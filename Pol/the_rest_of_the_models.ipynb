{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c8a9a8-b7ad-48ad-a2d0-dd252222ec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (701648, 117)\n",
      "\n",
      "Class distribution:\n",
      " label\n",
      "Mirai     652100\n",
      "Benign     49548\n",
      "Name: count, dtype: int64\n",
      "\n",
      "X shape: (701648, 115)  | y shape: (701648,)\n",
      "Num features: 115\n",
      "\n",
      "After NearMiss - X_train: (69368, 115)  y_train: (array(['Benign', 'Mirai'], dtype=object), array([34684, 34684]))\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "\n",
    "\n",
    "\n",
    "DATA_PATH  = \"rawdata/combined_iot_data.csv\"  \n",
    "TARGET_COL = \"label\"                           \n",
    "\n",
    "DROP_COLS  = [\"device\"]                       \n",
    "\n",
    "TEST_SIZE  = 0.30\n",
    "RANDOM_STATE = 47\n",
    "\n",
    "DO_BALANCE = True \n",
    "DO_SCALE   = True  \n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nClass distribution:\\n\", df[TARGET_COL].value_counts().head(20))\n",
    "\n",
    "df = df.dropna(subset=[TARGET_COL]).copy()\n",
    "\n",
    "drop_now = [c for c in ([TARGET_COL] + DROP_COLS) if c in df.columns]\n",
    "X_df = df.drop(columns=drop_now).copy()\n",
    "y = df[TARGET_COL].astype(str).values\n",
    "\n",
    "cat_cols = X_df.select_dtypes(include=[\"object\"]).columns\n",
    "for c in cat_cols:\n",
    "    X_df[c] = LabelEncoder().fit_transform(X_df[c].astype(str))\n",
    "\n",
    "X_df = X_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "X = X_df.values\n",
    "feature_names = list(X_df.columns)\n",
    "\n",
    "print(\"\\nX shape:\", X.shape, \" | y shape:\", y.shape)\n",
    "print(\"Num features:\", len(feature_names))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "if DO_BALANCE:\n",
    "    nm = NearMiss()\n",
    "    X_train, y_train = nm.fit_resample(X_train, y_train)\n",
    "    print(\"\\nAfter NearMiss - X_train:\", X_train.shape, \" y_train:\", np.unique(y_train, return_counts=True))\n",
    "\n",
    "scaler = None\n",
    "if DO_SCALE:\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(name)\n",
    "    print(\"=\"*70)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 (weighted):\", f1)\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    return {\"name\": name, \"acc\": acc, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40159e4e-ddc5-478f-bbc2-f7125023f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Logistic Regression\n",
      "======================================================================\n",
      "Accuracy: 0.7144777785695622\n",
      "F1 (weighted): 0.7840843358587943\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 14861      3]\n",
      " [ 60098 135533]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.20      1.00      0.33     14864\n",
      "       Mirai       1.00      0.69      0.82    195631\n",
      "\n",
      "    accuracy                           0.71    210495\n",
      "   macro avg       0.60      0.85      0.57    210495\n",
      "weighted avg       0.94      0.71      0.78    210495\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Decision Tree\n",
      "======================================================================\n",
      "Accuracy: 0.8677355756668804\n",
      "F1 (weighted): 0.8946515744969729\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 14864      0]\n",
      " [ 27841 167790]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.35      1.00      0.52     14864\n",
      "       Mirai       1.00      0.86      0.92    195631\n",
      "\n",
      "    accuracy                           0.87    210495\n",
      "   macro avg       0.67      0.93      0.72    210495\n",
      "weighted avg       0.95      0.87      0.89    210495\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Random Forest\n",
      "======================================================================\n",
      "Accuracy: 0.9657426542198152\n",
      "F1 (weighted): 0.9687647957895111\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 14864      0]\n",
      " [  7211 188420]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.67      1.00      0.80     14864\n",
      "       Mirai       1.00      0.96      0.98    195631\n",
      "\n",
      "    accuracy                           0.97    210495\n",
      "   macro avg       0.84      0.98      0.89    210495\n",
      "weighted avg       0.98      0.97      0.97    210495\n",
      "\n",
      "\n",
      "======================================================================\n",
      "XGBoost\n",
      "======================================================================\n",
      "Accuracy: 0.9970213069194043\n",
      "F1 (weighted): 0.9970496797108711\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 14864      0]\n",
      " [   627 195004]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.96      1.00      0.98     14864\n",
      "       Mirai       1.00      1.00      1.00    195631\n",
      "\n",
      "    accuracy                           1.00    210495\n",
      "   macro avg       0.98      1.00      0.99    210495\n",
      "weighted avg       1.00      1.00      1.00    210495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "lr = LogisticRegression(max_iter=2000, n_jobs=-1)\n",
    "lr.fit(X_train, y_train)\n",
    "results.append(evaluate_model(\"Logistic Regression\", lr, X_test, y_test))\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "dt.fit(X_train, y_train)\n",
    "results.append(evaluate_model(\"Decision Tree\", dt, X_test, y_test))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "results.append(evaluate_model(\"Random Forest\", rf, X_test, y_test))\n",
    "\n",
    "if HAS_XGB:\n",
    "    le_y = LabelEncoder()\n",
    "    y_train_enc = le_y.fit_transform(y_train)\n",
    "    y_test_enc  = le_y.transform(y_test)\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    xgb.fit(X_train, y_train_enc)\n",
    "\n",
    "    y_pred_enc = xgb.predict(X_test)\n",
    "    y_pred = le_y.inverse_transform(y_pred_enc)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"XGBoost\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 (weighted):\", f1)\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    results.append({\"name\": \"XGBoost\", \"acc\": acc, \"f1\": f1})\n",
    "\n",
    "else:\n",
    "    print(\"\\nXGBoost no disponible. Instala con: pip install xgboost\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ed6074c-f2b6-40b8-9fe9-df08f6e958c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Extra Trees\n",
      "======================================================================\n",
      "Accuracy: 0.9995344307465736\n",
      "F1 (weighted): 0.9995351372920427\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 14864      0]\n",
      " [    98 195533]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.99      1.00      1.00     14864\n",
      "       Mirai       1.00      1.00      1.00    195631\n",
      "\n",
      "    accuracy                           1.00    210495\n",
      "   macro avg       1.00      1.00      1.00    210495\n",
      "weighted avg       1.00      1.00      1.00    210495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et = ExtraTreesClassifier(\n",
    "    n_estimators=500,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "et.fit(X_train, y_train)\n",
    "results.append(evaluate_model(\"Extra Trees\", et, X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d17158-83c8-4633-9ff3-e7026be823ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.0366 - val_accuracy: 1.0000 - val_loss: 6.8801e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 1.0210e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 3.0062e-06\n",
      "Epoch 4/10\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 7.0250e-04 - val_accuracy: 1.0000 - val_loss: 4.6617e-07\n",
      "Epoch 5/10\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 1.9095e-04 - val_accuracy: 1.0000 - val_loss: 2.4108e-07\n",
      "Epoch 6/10\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 2.0604e-04 - val_accuracy: 1.0000 - val_loss: 1.9976e-07\n",
      "Epoch 7/10\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 1.3185e-04 - val_accuracy: 1.0000 - val_loss: 2.0457e-07\n",
      "Epoch 8/10\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 1.5140e-04 - val_accuracy: 1.0000 - val_loss: 4.3252e-07\n",
      "Epoch 9/10\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 2.2968e-04 - val_accuracy: 1.0000 - val_loss: 3.5114e-07\n",
      "Epoch 10/10\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 1.4723e-04 - val_accuracy: 1.0000 - val_loss: 2.9953e-08\n",
      "\n",
      "======================================================================\n",
      "Neural Network (Keras)\n",
      "======================================================================\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 14861      3]\n",
      " [ 60161 135470]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.20      1.00      0.33     14864\n",
      "       Mirai       1.00      0.69      0.82    195631\n",
      "\n",
      "    accuracy                           0.71    210495\n",
      "   macro avg       0.60      0.85      0.57    210495\n",
      "weighted avg       0.94      0.71      0.78    210495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = sorted(list(np.unique(y_train)))\n",
    "n_classes = len(classes)\n",
    "\n",
    "\n",
    "label_to_id = {c:i for i,c in enumerate(classes)}\n",
    "y_train_i = np.array([label_to_id[v] for v in y_train], dtype=np.int32)\n",
    "y_test_i  = np.array([label_to_id[v] for v in y_test],  dtype=np.int32)\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Input(shape=(X_train.shape[1],)))\n",
    "nn.add(Dense(64, activation=\"relu\"))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Dense(32, activation=\"relu\"))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Dense(16, activation=\"relu\"))\n",
    "\n",
    "if n_classes == 2:\n",
    "    nn.add(Dense(1, activation=\"sigmoid\"))\n",
    "    nn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    nn.fit(X_train, (y_train_i == 1).astype(np.float32), validation_split=0.2, epochs=10, batch_size=256, verbose=1)\n",
    "    y_pred_prob = nn.predict(X_test, verbose=0).ravel()\n",
    "    y_pred_i = (y_pred_prob >= 0.5).astype(int)\n",
    "else:\n",
    "    nn.add(Dense(n_classes, activation=\"softmax\"))\n",
    "    nn.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    nn.fit(X_train, y_train_i, validation_split=0.2, epochs=10, batch_size=256, verbose=1)\n",
    "    y_pred_i = nn.predict(X_test, verbose=0).argmax(axis=1)\n",
    "\n",
    "# vuelve a etiquetas originales\n",
    "id_to_label = {i:c for c,i in label_to_id.items()}\n",
    "y_pred = np.array([id_to_label[i] for i in y_pred_i])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Neural Network (Keras)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
