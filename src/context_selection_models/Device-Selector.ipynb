{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ca0d70-e754-4eb6-8a11-3e74a904e7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " TRAINING CONTEXT CLASSIFIER \n",
      "======================================================================\n",
      "\n",
      " Processing: Doorbell\n",
      "======================================================================\n",
      "  [1/6] ack... ..  10 chunks (~49% sampled)\n",
      "  [2/6] benign_traffic... .  5 chunks (~50% sampled)\n",
      "  [3/6] scan... ...  11 chunks (~51% sampled)\n",
      "  [4/6] syn... ...  12 chunks (~49% sampled)\n",
      "  [5/6] udp... .....  24 chunks (~50% sampled)\n",
      "  [6/6] udpplain... ..  8 chunks (~49% sampled)\n",
      "\n",
      " Doorbell: 70 samples | Avg coverage: ~50%\n",
      "\n",
      " Processing: Other\n",
      "======================================================================\n",
      "  [1/13] BenignTraffic.pcap... ........  36 chunks (~50% sampled)\n",
      "  [2/13] Mirai-greeth_flood2.pcap... .  3 chunks (~44% sampled)\n",
      "  [3/13] Mirai-greeth_flood3.pcap... .  3 chunks (~44% sampled)\n",
      "  [4/13] Mirai-greeth_flood4.pcap... .  3 chunks (~44% sampled)\n",
      "  [5/13] Mirai-greip_flood1.pcap... .  4 chunks (~57% sampled)\n",
      "  [6/13] Mirai-greip_flood2.pcap... .  3 chunks (~43% sampled)\n",
      "  [7/13] Mirai-greip_flood3.pcap... .  4 chunks (~57% sampled)\n",
      "  [8/13] Mirai-greip_flood4.pcap... .  4 chunks (~56% sampled)\n",
      "  [9/13] Mirai-udpplain.pcap... .  4 chunks (~55% sampled)\n",
      "  [10/13] Mirai-udpplain1.pcap... .  4 chunks (~55% sampled)\n",
      "  [11/13] Mirai-udpplain2.pcap... .  4 chunks (~55% sampled)\n",
      "  [12/13] Mirai-udpplain3.pcap... .  4 chunks (~54% sampled)\n",
      "  [13/13] Mirai-udpplain4.pcap... .  4 chunks (~55% sampled)\n",
      "\n",
      " Other: 80 samples | Avg coverage: ~51%\n",
      "\n",
      " Dataset generated: (150, 48)\n",
      "   Balance: Doorbell=70, Other=80\n",
      "\n",
      " Normalizing features...\n",
      " Splitting train/test (80/20)...\n",
      "   Train: 120 | Test: 30\n",
      "\n",
      " Training Random Forest...\n",
      "   Complete\n",
      "\n",
      " TEST SET EVALUATION:\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13  1]\n",
      " [ 0 16]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Doorbell       1.00      0.93      0.96        14\n",
      "       Other       0.94      1.00      0.97        16\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "\n",
      " Cross-Validation (5-fold)...\n",
      "   Accuracy: 99.33% ± 1.33%\n",
      "   Individual scores: ['100.0%', '96.7%', '100.0%', '100.0%', '100.0%']\n",
      "     feature  importance\n",
      "   mean.mean    0.124667\n",
      "    max.mean    0.110000\n",
      "  range.mean    0.080000\n",
      "     sd.mean    0.071935\n",
      "      max.sd    0.070000\n",
      "     nr_attr    0.063408\n",
      "    range.sd    0.063268\n",
      "attr_to_inst    0.060712\n",
      "       sd.sd    0.053069\n",
      "inst_to_attr    0.051014\n",
      "\n",
      " Saving model...\n",
      "  device_selector_classifier_pymfe.pkl\n",
      "  device_selector_scaler_pymfe.pkl\n",
      "\n",
      "======================================================================\n",
      "COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymfe.mfe import MFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "BASE_PATH = 'selector-data'\n",
    "\n",
    "# Doorbell-data (Context 1)\n",
    "DOORBELL_PATH = os.path.join(BASE_PATH, 'Doorbell-data')\n",
    "DOORBELL_FILES = ['ack', 'benign_traffic', 'scan', 'syn', 'udp', 'udpplain']\n",
    "\n",
    "# Other-devices-data (Context 2)\n",
    "OTHER_PATH = os.path.join(BASE_PATH, 'Other-devices-data')\n",
    "OTHER_FILES = [\n",
    "    'BenignTraffic.pcap',\n",
    "    'Mirai-greeth_flood2.pcap',\n",
    "    'Mirai-greeth_flood3.pcap',\n",
    "    'Mirai-greeth_flood4.pcap',\n",
    "    'Mirai-greip_flood1.pcap',\n",
    "    'Mirai-greip_flood2.pcap',\n",
    "    'Mirai-greip_flood3.pcap',\n",
    "    'Mirai-greip_flood4.pcap',\n",
    "    'Mirai-udpplain.pcap',\n",
    "    'Mirai-udpplain1.pcap',\n",
    "    'Mirai-udpplain2.pcap',\n",
    "    'Mirai-udpplain3.pcap',\n",
    "    'Mirai-udpplain4.pcap'\n",
    "]\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 5000          \n",
    "STRIDE = 10000             \n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Extract meta-features using ONLY pymfe\n",
    "# ============================================================================\n",
    "def extract_metafeatures(df):\n",
    "\n",
    "    try:\n",
    "        # Initialize pymfe\n",
    "        mfe = MFE(\n",
    "            groups=[\"general\", \"statistical\"],\n",
    "            summary=[\"mean\", \"sd\"]\n",
    "        )\n",
    "        \n",
    "        # Fit on the data (unsupervised)\n",
    "        mfe.fit(df.values, None)\n",
    "        \n",
    "        # Extract features\n",
    "        feature_names, feature_values = mfe.extract()\n",
    "        \n",
    "        # Convert to dict\n",
    "        meta = dict(zip(feature_names, feature_values))\n",
    "        \n",
    "        # Handle inf/nan from pymfe\n",
    "        for key in meta:\n",
    "            if not np.isfinite(meta[key]):\n",
    "                meta[key] = 0.0\n",
    "        \n",
    "        return meta\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Minimal fallback\n",
    "        return {\n",
    "            'nr_attr': df.shape[1],\n",
    "            'nr_inst': len(df),\n",
    "            'mean': df.mean().mean() if df.shape[1] > 0 else 0\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Process file into chunks (with sampling)\n",
    "# ============================================================================\n",
    "def process_file_chunks(file_path, chunk_size, stride):\n",
    "\n",
    "    samples = []\n",
    "    \n",
    "    # Load file\n",
    "    df = pd.read_csv(file_path)\n",
    "    file_size = len(df)\n",
    "    \n",
    "    # Process in chunks with stride\n",
    "    n_chunks = 0\n",
    "    for i in range(0, file_size - chunk_size + 1, stride):\n",
    "        chunk = df.iloc[i:i + chunk_size]\n",
    "        \n",
    "        # Extract meta-features\n",
    "        if n_chunks % 5 == 0:  # Progress every 5 chunks\n",
    "            print(f\".\", end=\"\", flush=True)\n",
    "        \n",
    "        meta = extract_metafeatures(chunk)\n",
    "        samples.append(meta)\n",
    "        n_chunks += 1\n",
    "    \n",
    "    # Calculate coverage\n",
    "    rows_processed = n_chunks * chunk_size\n",
    "    coverage = min(100, (rows_processed / file_size) * 100)\n",
    "    \n",
    "    return samples, coverage\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Process datasets with chunking\n",
    "# ============================================================================\n",
    "def process_datasets(file_list, base_path, label):\n",
    "\n",
    "    X_samples = []\n",
    "    y_labels = []\n",
    "    \n",
    "    print(f\"\\n Processing: {label}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_files_coverage = []\n",
    "    \n",
    "    for idx, filename in enumerate(file_list, 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Try multiple extensions\n",
    "        paths = [\n",
    "            os.path.join(base_path, filename),\n",
    "            os.path.join(base_path, filename + '.csv'),\n",
    "            os.path.join(base_path, filename + '.pcap.csv')\n",
    "        ]\n",
    "        \n",
    "        file_path = None\n",
    "        for p in paths:\n",
    "            if os.path.exists(p):\n",
    "                file_path = p\n",
    "                break\n",
    "        \n",
    "        if not file_path:\n",
    "            print(f\"  [{idx}/{len(file_list)}]   {filename} - Not found\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"  [{idx}/{len(file_list)}] {filename}... \", end=\"\", flush=True)\n",
    "            \n",
    "            samples, coverage = process_file_chunks(file_path, CHUNK_SIZE, STRIDE)\n",
    "            total_files_coverage.append(coverage)\n",
    "            \n",
    "            # Add to training set\n",
    "            for meta in samples:\n",
    "                X_samples.append(list(meta.values()))\n",
    "                y_labels.append(label)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"  {len(samples)} chunks (~{coverage:.0f}% sampled)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  [{idx}/{len(file_list)}]  {filename}: {e}\")\n",
    "    \n",
    "    avg_coverage = np.mean(total_files_coverage) if total_files_coverage else 0\n",
    "    print(f\"\\n {label}: {len(y_labels)} samples | Avg coverage: ~{avg_coverage:.0f}%\")\n",
    "    return X_samples, y_labels\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\" TRAINING CONTEXT CLASSIFIER \")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "# Process both contexts\n",
    "X_doorbell, y_doorbell = process_datasets(DOORBELL_FILES, DOORBELL_PATH, 'Doorbell')\n",
    "X_other, y_other = process_datasets(OTHER_FILES, OTHER_PATH, 'Other')\n",
    "\n",
    "# Combine datasets\n",
    "X = np.array(X_doorbell + X_other)\n",
    "y = np.array(y_doorbell + y_other)\n",
    "\n",
    "print(f\"\\n Dataset generated: {X.shape}\")\n",
    "print(f\"   Balance: Doorbell={sum(y=='Doorbell')}, Other={sum(y=='Other')}\")\n",
    "\n",
    "# Check for inf/nan\n",
    "if not np.isfinite(X).all():\n",
    "    print(\"\\n  Cleaning inf/nan values...\")\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "    print(\"    Cleaned\")\n",
    "\n",
    "if len(X) < 10:\n",
    "    print(\"\\n ERROR: Too few samples generated\")\n",
    "else:\n",
    "    # Normalize features\n",
    "    print(\"\\n Normalizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Train/test split\n",
    "    print(\" Splitting train/test (80/20)...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    print(f\"   Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "    \n",
    "    # Train Random Forest\n",
    "    print(\"\\n Training Random Forest...\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"   Complete\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\n TEST SET EVALUATION:\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Cross-validation\n",
    "    print(\"\\n Cross-Validation (5-fold)...\")\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=5, n_jobs=-1)\n",
    "    print(f\"   Accuracy: {scores.mean():.2%} ± {scores.std():.2%}\")\n",
    "    print(f\"   Individual scores: {[f'{s:.1%}' for s in scores]}\")\n",
    "    \n",
    "    \n",
    "    if len(feature_names) == X.shape[1]:\n",
    "        importances = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(importances.head(10).to_string(index=False))\n",
    "    \n",
    "    # Save model\n",
    "    print(\"\\n Saving model...\")\n",
    "    joblib.dump(model, 'device_selector_classifier_pymfe.pkl')\n",
    "    joblib.dump(scaler, 'device_selector_scaler_pymfe.pkl')\n",
    "    print(\"  device_selector_classifier_pymfe.pkl\")\n",
    "    print(\"  device_selector_scaler_pymfe.pkl\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6ee4ad1-97e0-410b-b32a-e6c2baebd456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model and scaler...\n",
      "✓ Model loaded successfully\n",
      "\n",
      "Reading test file: Mirai-greip_flood19.pcap.csv\n",
      " Loaded dataset with 34844 rows and 39 features\n",
      "\n",
      "Extracting meta-features...\n",
      "✓ Extracted 48 meta-features\n",
      "\n",
      "Running prediction...\n",
      "\n",
      "============================================================\n",
      "PREDICTION RESULTS\n",
      "============================================================\n",
      "\n",
      "Predicted Class: Other\n",
      "Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# DEVICE CLASSIFIER - PREDICTION TEST\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pymfe.mfe import MFE\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIGURATION\n",
    "MODEL_PATH = 'models/device_selector_classifier_pymfe.pkl'\n",
    "SCALER_PATH = 'models/device_selector_scaler_pymfe.pkl'\n",
    "TEST_FILE = 'Mirai-greip_flood19.pcap.csv'\n",
    "\n",
    "# LOAD MODEL AND PREPROCESSING PIPELINE\n",
    "print(\"Loading trained model and scaler...\")\n",
    "model = joblib.load(MODEL_PATH)\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "print(\"✓ Model loaded successfully\\n\")\n",
    "\n",
    "# LOAD TEST DATA\n",
    "\n",
    "print(f\"Reading test file: {TEST_FILE}\")\n",
    "df = pd.read_csv(TEST_FILE)\n",
    "print(f\" Loaded dataset with {df.shape[0]} rows and {df.shape[1]} features\\n\")\n",
    "\n",
    "# EXTRACT META-FEATURES\n",
    "print(\"Extracting meta-features...\")\n",
    "mfe = MFE(\n",
    "    groups=[\"general\", \"statistical\"], \n",
    "    summary=[\"mean\", \"sd\"],\n",
    "    suppress_warnings=True  # Suppress pymfe internal warnings\n",
    ")\n",
    "\n",
    "# Fit and extract features\n",
    "mfe.fit(df.values, None)\n",
    "feature_names, feature_values = mfe.extract()\n",
    "\n",
    "# Clean infinite and NaN values\n",
    "feature_values = [0.0 if not np.isfinite(v) else v for v in feature_values]\n",
    "print(f\"✓ Extracted {len(feature_values)} meta-features\\n\")\n",
    "\n",
    "# NORMALIZE AND PREDICT\n",
    "print(\"Running prediction...\")\n",
    "\n",
    "# Prepare input for model\n",
    "X_new = np.array([feature_values])\n",
    "X_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Get prediction and probabilities\n",
    "prediction = model.predict(X_scaled)[0]\n",
    "probabilities = model.predict_proba(X_scaled)[0]\n",
    "confidence = probabilities.max()\n",
    "\n",
    "# DISPLAY RESULTS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPredicted Class: {prediction}\")\n",
    "print(f\"Confidence: {confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b8f75-1691-477d-b37f-ec6580706f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
